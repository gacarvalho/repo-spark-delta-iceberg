2024-09-10T16:51:53,487 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-10T16:51:53,493 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-1db873b0-bddf-416f-87f1-cc457b7edc55
2024-09-10T16:52:43,693 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-10T16:52:43,697 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-10T16:52:43,697 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-10T16:52:43,728 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-10T16:52:43,729 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-10T16:52:43,730 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-10T16:52:43,730 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-rides-fhvhv-py-berry
2024-09-10T16:52:43,765 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-10T16:52:43,781 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-10T16:52:43,783 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-10T16:52:43,854 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-10T16:52:43,855 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-10T16:52:43,856 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-10T16:52:43,857 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-10T16:52:43,857 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-10T16:52:43,940 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-10T16:52:44,223 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 34099.
2024-09-10T16:52:44,268 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-10T16:52:44,329 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-10T16:52:44,358 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-10T16:52:44,359 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-10T16:52:44,366 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-10T16:52:44,417 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-01691ce7-e041-46a5-8730-3e035ddb26d9
2024-09-10T16:52:44,439 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-10T16:52:44,463 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-10T16:52:44,528 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3868ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-10T16:52:44,675 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-10T16:52:44,694 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-10T16:52:44,725 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4065ms
2024-09-10T16:52:44,779 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@71ff7a40{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-10T16:52:44,779 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-10T16:52:44,819 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4dbe32{/,null,AVAILABLE,@Spark}
2024-09-10T16:52:44,970 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-10T16:52:45,046 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 52 ms (0 ms spent in bootstraps)
2024-09-10T16:52:45,275 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240910165245-0000
2024-09-10T16:52:45,287 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34281.
2024-09-10T16:52:45,287 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:34281
2024-09-10T16:52:45,291 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-10T16:52:45,309 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 34281, None)
2024-09-10T16:52:45,312 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240910165245-0000/0 on worker-20240910162622-172.19.0.5-38211 (172.19.0.5:38211) with 2 core(s)
2024-09-10T16:52:45,316 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:34281 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 34281, None)
2024-09-10T16:52:45,319 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240910165245-0000/0 on hostPort 172.19.0.5:38211 with 2 core(s), 3.0 GiB RAM
2024-09-10T16:52:45,320 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240910165245-0000/1 on worker-20240910162604-172.19.0.3-33339 (172.19.0.3:33339) with 2 core(s)
2024-09-10T16:52:45,321 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240910165245-0000/1 on hostPort 172.19.0.3:33339 with 2 core(s), 3.0 GiB RAM
2024-09-10T16:52:45,322 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240910165245-0000/2 on worker-20240910162613-172.19.0.4-38475 (172.19.0.4:38475) with 2 core(s)
2024-09-10T16:52:45,324 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240910165245-0000/2 on hostPort 172.19.0.4:38475 with 2 core(s), 3.0 GiB RAM
2024-09-10T16:52:45,328 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 34281, None)
2024-09-10T16:52:45,336 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 34281, None)
2024-09-10T16:52:45,847 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240910165245-0000/2 is now RUNNING
2024-09-10T16:52:45,858 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240910165245-0000/0 is now RUNNING
2024-09-10T16:52:45,860 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240910165245-0000/1 is now RUNNING
2024-09-10T16:52:46,352 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240910165245-0000.inprogress
2024-09-10T16:52:46,888 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@4dbe32{/,null,STOPPED,@Spark}
2024-09-10T16:52:46,897 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d1c8492{/jobs,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,905 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36e845c1{/jobs/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,912 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5254688f{/jobs/job,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,921 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@32fe796c{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,932 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52d6bc0a{/stages,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,941 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@32b34475{/stages/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,952 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42cbff06{/stages/stage,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,961 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29ffb41d{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,973 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a33a891{/stages/pool,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,977 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@10dc4c4f{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,979 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a89c27d{/storage,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,983 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7650db27{/storage/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,986 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@395355c{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,988 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15187820{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,992 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@54c2c8cf{/environment,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,994 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6fb3a9be{/environment/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:46,997 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a09f37f{/executors,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,000 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@419e0aed{/executors/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,006 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@32a04c5b{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,009 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@25b7d19a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,012 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3cc50417{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,015 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4801a064{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,038 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a6b6161{/static,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,042 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d9ad25{/,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,061 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a3ccae9{/api,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,065 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@f9f546{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,072 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ce1f968{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,085 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@516ba7d4{/metrics/json,null,AVAILABLE,@Spark}
2024-09-10T16:52:47,090 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:34:13,940 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:34:13,957 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:34:13,958 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:34:14,016 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:34:14,017 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:34:14,018 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:34:14,019 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:34:14,057 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:34:14,071 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:34:14,072 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:34:14,167 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:34:14,168 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:34:14,169 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:34:14,170 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:34:14,171 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:34:14,274 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:34:14,632 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44773.
2024-09-11T01:34:14,686 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:34:14,749 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:34:14,786 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:34:14,787 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:34:14,792 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:34:14,840 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-06ccc0fd-dfcd-4393-bf4c-4128c2a6c761
2024-09-11T01:34:14,862 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:34:14,885 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:34:14,950 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4792ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:34:15,065 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:34:15,084 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:34:15,114 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4956ms
2024-09-11T01:34:15,159 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@64781924{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:34:15,160 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:34:15,199 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31eb2684{/,null,AVAILABLE,@Spark}
2024-09-11T01:34:15,348 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:34:15,416 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 42 ms (0 ms spent in bootstraps)
2024-09-11T01:34:15,647 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911013415-0000
2024-09-11T01:34:15,661 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41143.
2024-09-11T01:34:15,662 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:41143
2024-09-11T01:34:15,666 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:34:15,682 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 41143, None)
2024-09-11T01:34:15,691 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:41143 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 41143, None)
2024-09-11T01:34:15,697 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 41143, None)
2024-09-11T01:34:15,700 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911013415-0000/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:34:15,703 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 41143, None)
2024-09-11T01:34:15,707 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911013415-0000/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:34:15,708 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911013415-0000/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:34:15,710 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911013415-0000/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:34:15,719 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911013415-0000/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:34:15,720 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911013415-0000/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:34:16,218 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911013415-0000/1 is now RUNNING
2024-09-11T01:34:16,224 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911013415-0000/0 is now RUNNING
2024-09-11T01:34:16,226 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911013415-0000/2 is now RUNNING
2024-09-11T01:34:16,801 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911013415-0000.inprogress
2024-09-11T01:34:17,330 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@31eb2684{/,null,STOPPED,@Spark}
2024-09-11T01:34:17,337 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e4df089{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,343 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d59c19{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,349 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3fd1cc40{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,351 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5df4d28e{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,358 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51f87848{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,365 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c1601e3{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,375 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79a7bf48{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,384 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4d269d2b{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,389 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c2197ef{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,403 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4999673e{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,436 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40d4991f{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,447 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@94c9557{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,459 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ae9f084{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,469 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@696efd87{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,477 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e447cfb{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,488 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7da35649{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d1a065a{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,502 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7123b375{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,507 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6caa1c1c{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,509 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3be77417{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,513 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49a59da6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,516 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@747575{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,602 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9c93455{/static,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,606 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@60eb05d6{/,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,617 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5612616b{/api,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,628 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41e3bce6{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,637 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c7a21f5{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,656 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@bd4080{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:34:17,662 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:34:32,307 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:34:32,318 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:41:10,227 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:41:10,231 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:41:10,232 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:41:10,274 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:41:10,275 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:41:10,276 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:41:10,277 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:41:10,306 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:41:10,321 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:41:10,323 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:41:10,384 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:41:10,385 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:41:10,386 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:41:10,387 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:41:10,387 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:41:10,465 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:41:10,803 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 38453.
2024-09-11T01:41:10,865 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:41:10,919 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:41:10,960 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:41:10,961 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:41:10,968 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:41:11,004 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-56ec8cb2-9293-4b1a-8495-be7ff7c6e760
2024-09-11T01:41:11,031 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:41:11,050 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:41:11,108 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3848ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:41:11,214 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:41:11,225 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:41:11,249 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3991ms
2024-09-11T01:41:11,307 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@428f7fd5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:41:11,307 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:41:11,359 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4fdd5d3e{/,null,AVAILABLE,@Spark}
2024-09-11T01:41:11,505 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:41:11,568 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2024-09-11T01:41:11,691 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911014111-0001
2024-09-11T01:41:11,699 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014111-0001/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:41:11,707 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014111-0001/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:41:11,708 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014111-0001/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:41:11,709 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014111-0001/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:41:11,710 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014111-0001/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:41:11,711 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014111-0001/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:41:11,720 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34795.
2024-09-11T01:41:11,722 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:34795
2024-09-11T01:41:11,727 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:41:11,743 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 34795, None)
2024-09-11T01:41:11,757 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:34795 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 34795, None)
2024-09-11T01:41:11,775 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 34795, None)
2024-09-11T01:41:11,780 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 34795, None)
2024-09-11T01:41:11,847 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014111-0001/1 is now RUNNING
2024-09-11T01:41:11,848 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014111-0001/0 is now RUNNING
2024-09-11T01:41:11,852 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014111-0001/2 is now RUNNING
2024-09-11T01:41:12,347 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911014111-0001.inprogress
2024-09-11T01:41:12,744 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@4fdd5d3e{/,null,STOPPED,@Spark}
2024-09-11T01:41:12,755 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d07fdab{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,758 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ce267e7{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,770 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@33a12365{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,780 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43bb3020{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,788 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@787b4aab{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,793 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7838ad98{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,799 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1e8964ef{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,801 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a3b14c7{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,805 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f11f5f0{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,810 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2a2f34aa{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,813 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22316a49{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,818 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dc6bb5d{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,825 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3bd03fbd{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,836 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@27dd3738{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,842 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5c757fa4{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,847 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@264c82e3{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,850 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@23eb68d8{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,854 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@392d3986{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,857 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3030a6d3{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,860 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3c556260{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,875 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6cb3344a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,879 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@33f5d9b6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,908 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ff4a40c{/static,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,915 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@135205b1{/,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,920 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b52c1fc{/api,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,925 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@656ad493{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,928 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@60366338{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,942 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@674ce066{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:41:12,949 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:41:26,960 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:41:26,972 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:41:29,419 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 4) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:41:29,531 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 5) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:41:30,701 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 6) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:41:30,792 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:41:30,794 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2024-09-11T01:41:30,805 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job d283954c-b7a6-42fa-8d13-f95428a8ca21.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:569) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/5587_stagemetrics/_temporary/0/_temporary/attempt_202409110141285721176204999489678_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014111-0001/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	... 1 more
2024-09-11T01:43:36,814 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:43:36,818 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:43:36,819 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:43:36,829 [Thread-4] WARN  org.apache.spark.SparkConf [] - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2024-09-11T01:43:36,873 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:43:36,874 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:43:36,874 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:43:36,875 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:43:36,908 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:43:36,918 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:43:36,920 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:43:36,991 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:43:36,991 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:43:36,992 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:43:36,993 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:43:36,994 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:43:37,063 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:43:37,389 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 43065.
2024-09-11T01:43:37,438 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:43:37,497 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:43:37,539 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:43:37,540 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:43:37,547 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:43:37,595 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-temp/blockmgr-a78255fa-1bf7-421e-9177-b9b22ad600ca
2024-09-11T01:43:37,617 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:43:37,642 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:43:37,698 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3926ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:43:37,800 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:43:37,813 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:43:37,839 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4068ms
2024-09-11T01:43:37,889 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@5b882473{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:43:37,890 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:43:37,925 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@228dbf59{/,null,AVAILABLE,@Spark}
2024-09-11T01:43:38,058 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:43:38,126 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 39 ms (0 ms spent in bootstraps)
2024-09-11T01:43:38,252 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911014338-0002
2024-09-11T01:43:38,255 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014338-0002/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:43:38,273 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014338-0002/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:43:38,274 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014338-0002/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:43:38,275 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014338-0002/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:43:38,276 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014338-0002/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:43:38,278 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014338-0002/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:43:38,284 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39309.
2024-09-11T01:43:38,284 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:39309
2024-09-11T01:43:38,288 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:43:38,317 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 39309, None)
2024-09-11T01:43:38,346 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:39309 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 39309, None)
2024-09-11T01:43:38,361 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 39309, None)
2024-09-11T01:43:38,372 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 39309, None)
2024-09-11T01:43:38,434 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014338-0002/2 is now RUNNING
2024-09-11T01:43:38,440 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014338-0002/1 is now RUNNING
2024-09-11T01:43:38,457 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014338-0002/0 is now RUNNING
2024-09-11T01:43:38,910 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911014338-0002.inprogress
2024-09-11T01:43:39,319 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@228dbf59{/,null,STOPPED,@Spark}
2024-09-11T01:43:39,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4a4e32d3{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,339 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cd3b247{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,343 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42fbf39a{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,353 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6298abdb{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,356 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65c2f626{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,366 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@602f6b66{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,372 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a6a584e{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,374 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@18809651{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,376 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22fa189{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,379 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3e9f5240{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,381 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@129a6c85{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,384 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1977d5{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,390 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@66629902{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,396 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@68661c20{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,399 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5414a5a1{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,402 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7b34e799{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,405 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4865baaf{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,407 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c648763{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,410 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59000787{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,417 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@24a4e920{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,424 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a90cc70{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,431 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c9ab33b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,463 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d205e7c{/static,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,468 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15d9a953{/,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,474 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@399a34f2{/api,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,477 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1463ce8b{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,480 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@662ac374{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,495 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@43a82bb5{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:43:39,497 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:43:51,028 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:43:51,050 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:43:53,629 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 4) (172.19.0.3 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:43:53,751 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 5) (172.19.0.3 executor 0): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:43:54,372 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 6) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:43:54,475 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:43:54,476 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2024-09-11T01:43:54,491 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job 58654e91-56dc-4e70-a150-c2a567c17f8e.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:569) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/2726_stagemetrics/_temporary/0/_temporary/attempt_202409110143527440784067486890640_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014338-0002/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	... 1 more
2024-09-11T01:44:55,238 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:44:55,243 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:44:55,243 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:44:55,249 [Thread-4] WARN  org.apache.spark.SparkConf [] - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2024-09-11T01:44:55,280 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:44:55,281 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:44:55,281 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:44:55,282 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:44:55,312 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:44:55,329 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:44:55,331 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:44:55,396 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:44:55,397 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:44:55,398 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:44:55,399 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:44:55,400 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:44:55,473 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:44:55,754 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 42105.
2024-09-11T01:44:55,787 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:44:55,831 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:44:55,867 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:44:55,868 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:44:55,873 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:44:55,903 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-temp/blockmgr-fcf7046f-2fc5-4a40-89a1-36bf870f892b
2024-09-11T01:44:55,918 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:44:55,944 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:44:55,996 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3593ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:44:56,095 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:44:56,110 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:44:56,136 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3734ms
2024-09-11T01:44:56,175 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@64781924{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:44:56,175 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:44:56,209 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31eb2684{/,null,AVAILABLE,@Spark}
2024-09-11T01:44:56,330 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:44:56,393 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 40 ms (0 ms spent in bootstraps)
2024-09-11T01:44:56,512 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911014456-0003
2024-09-11T01:44:56,516 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014456-0003/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:44:56,524 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014456-0003/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:44:56,525 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014456-0003/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:44:56,526 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014456-0003/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:44:56,526 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014456-0003/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:44:56,527 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014456-0003/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:44:56,535 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42467.
2024-09-11T01:44:56,539 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:42467
2024-09-11T01:44:56,544 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:44:56,561 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 42467, None)
2024-09-11T01:44:56,580 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:42467 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 42467, None)
2024-09-11T01:44:56,609 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 42467, None)
2024-09-11T01:44:56,621 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 42467, None)
2024-09-11T01:44:56,647 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014456-0003/0 is now RUNNING
2024-09-11T01:44:56,663 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014456-0003/2 is now RUNNING
2024-09-11T01:44:56,678 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014456-0003/1 is now RUNNING
2024-09-11T01:44:57,358 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911014456-0003.inprogress
2024-09-11T01:44:57,685 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@31eb2684{/,null,STOPPED,@Spark}
2024-09-11T01:44:57,688 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e4df089{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,694 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73d59c19{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,701 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3fd1cc40{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,707 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5df4d28e{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,710 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@51f87848{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,715 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c1601e3{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,722 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79a7bf48{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,728 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4d269d2b{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,735 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c2197ef{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,737 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4999673e{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,740 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40d4991f{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,743 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@94c9557{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,746 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ae9f084{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,749 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@696efd87{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,751 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e447cfb{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,754 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7da35649{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,757 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d1a065a{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,759 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7123b375{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,762 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6caa1c1c{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,765 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3be77417{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,768 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49a59da6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,775 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@747575{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,794 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9c93455{/static,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,797 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@60eb05d6{/,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,802 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5612616b{/api,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,807 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41e3bce6{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,809 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c7a21f5{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,823 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@bd4080{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:44:57,828 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:44:59,445 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 390@d276954469b5
2024-09-11T01:44:59,467 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:44:59,470 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:44:59,471 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:44:59,528 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 391@e9a308ff8d57
2024-09-11T01:44:59,546 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:44:59,548 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:44:59,549 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:44:59,608 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 379@0c0fd82825b2
2024-09-11T01:44:59,630 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:44:59,633 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:44:59,634 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:45:00,320 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:45:00,392 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:45:00,490 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:45:00,610 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:00,612 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:00,613 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:00,614 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:00,616 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:00,622 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:00,623 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:00,624 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:00,626 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:00,627 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:00,804 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:00,806 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:00,807 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:00,809 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:00,810 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:01,457 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 209 ms (0 ms spent in bootstraps)
2024-09-11T01:45:01,463 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 215 ms (0 ms spent in bootstraps)
2024-09-11T01:45:01,594 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 189 ms (0 ms spent in bootstraps)
2024-09-11T01:45:01,752 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:01,753 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:01,753 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:01,753 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:01,754 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:01,762 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:01,763 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:01,764 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:01,764 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:01,767 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:01,865 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:45:01,866 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:45:01,868 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:45:01,869 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:45:01,871 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:45:01,903 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 8 ms (0 ms spent in bootstraps)
2024-09-11T01:45:01,913 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 9 ms (0 ms spent in bootstraps)
2024-09-11T01:45:02,021 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42105 after 13 ms (0 ms spent in bootstraps)
2024-09-11T01:45:02,149 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5c92221-ba46-4e0a-bfb4-0aaf13cc2f0a/executor-3a31560f-2ed7-4dbd-b110-f2c35788030a/blockmgr-578a9c84-3071-4d67-b1de-1ead96e4d86b
2024-09-11T01:45:02,152 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c0da433b-5299-4d08-a536-be15ca700c08/executor-bc256174-24e3-4bed-8e9c-f0d2162260d0/blockmgr-ca92600f-9d2e-4a73-a370-134140d17577
2024-09-11T01:45:02,248 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:45:02,248 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-0aee848a-ff36-45a7-94c2-67c781486a40/executor-5569ef55-2427-4550-b68a-c15a2eb2c0a6/blockmgr-77448531-5885-4b6c-ad7d-58cfb10c0d02
2024-09-11T01:45:02,249 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:45:02,337 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:45:02,805 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:42105
2024-09-11T01:45:02,807 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.5:45933
2024-09-11T01:45:02,832 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:42105
2024-09-11T01:45:02,839 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.4:36373
2024-09-11T01:45:02,848 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:36373 after 6 ms (0 ms spent in bootstraps)
2024-09-11T01:45:02,849 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:42105
2024-09-11T01:45:02,854 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:45933 after 45 ms (0 ms spent in bootstraps)
2024-09-11T01:45:02,852 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.3:43735
2024-09-11T01:45:02,864 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.5:45933
2024-09-11T01:45:02,867 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.4:36373
2024-09-11T01:45:02,868 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:43735 after 7 ms (0 ms spent in bootstraps)
2024-09-11T01:45:02,877 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,878 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:45:02,882 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,883 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:45:02,884 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,879 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,881 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.3:43735
2024-09-11T01:45:02,895 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,896 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:45:02,897 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:45:02,972 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:45:02,977 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:45:02,979 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.19.0.4
2024-09-11T01:45:02,980 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:45:02,982 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:45:02,983 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.19.0.3
2024-09-11T01:45:02,985 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:45:02,987 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:45:03,001 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:45:03,009 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.19.0.5
2024-09-11T01:45:03,012 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:45:03,017 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:45:03,087 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34537.
2024-09-11T01:45:03,088 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.3:34537
2024-09-11T01:45:03,089 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32867.
2024-09-11T01:45:03,091 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.4:32867
2024-09-11T01:45:03,092 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:45:03,094 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:45:03,108 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.19.0.3, 34537, None)
2024-09-11T01:45:03,110 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.19.0.4, 32867, None)
2024-09-11T01:45:03,118 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43865.
2024-09-11T01:45:03,119 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.5:43865
2024-09-11T01:45:03,126 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:45:03,140 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.19.0.3, 34537, None)
2024-09-11T01:45:03,140 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.19.0.4, 32867, None)
2024-09-11T01:45:03,143 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.19.0.4, 32867, None)
2024-09-11T01:45:03,143 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.19.0.5, 43865, None)
2024-09-11T01:45:03,144 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.19.0.3, 34537, None)
2024-09-11T01:45:03,159 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:45:03,165 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@50dcf711 for default.
2024-09-11T01:45:03,170 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:45:03,172 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@233aa7ab for default.
2024-09-11T01:45:03,193 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.19.0.5, 43865, None)
2024-09-11T01:45:03,197 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.19.0.5, 43865, None)
2024-09-11T01:45:03,217 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:45:03,219 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2ed18fe4 for default.
2024-09-11T01:45:04,919 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2024-09-11T01:45:04,929 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2024-09-11T01:45:05,035 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:05,102 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42467 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:45:05,164 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:45:05,181 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 145 ms
2024-09-11T01:45:05,261 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:45:07,314 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 295.286051 ms
2024-09-11T01:45:07,326 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1110, boot = 906, init = 204, finish = 0
2024-09-11T01:45:07,351 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 3177 bytes result sent to driver
2024-09-11T01:45:07,443 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2024-09-11T01:45:07,468 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2024-09-11T01:45:07,596 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:07,658 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42467 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:45:07,709 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:45:07,721 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 124 ms
2024-09-11T01:45:07,784 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:45:09,621 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 347.423159 ms
2024-09-11T01:45:09,635 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1212, boot = 938, init = 273, finish = 1
2024-09-11T01:45:09,667 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 3115 bytes result sent to driver
2024-09-11T01:45:11,183 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:45:11,196 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:45:11,563 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2024-09-11T01:45:11,563 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2024-09-11T01:45:11,564 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 2.0 (TID 2)
2024-09-11T01:45:11,564 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 2.0 (TID 3)
2024-09-11T01:45:11,578 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:11,579 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:11,595 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:45:11,600 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:45:11,607 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 28 ms
2024-09-11T01:45:11,610 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:45:11,614 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 34 ms
2024-09-11T01:45:11,620 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:45:11,789 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 2.0 (TID 2). 1665 bytes result sent to driver
2024-09-11T01:45:11,808 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 2.0 (TID 3). 1665 bytes result sent to driver
2024-09-11T01:45:12,076 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2024-09-11T01:45:12,077 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 4)
2024-09-11T01:45:12,097 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:45:12,101 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:12,114 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.5 KiB, free 1663.1 MiB)
2024-09-11T01:45:12,118 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 17 ms
2024-09-11T01:45:12,121 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 232.2 KiB, free 1662.9 MiB)
2024-09-11T01:45:12,280 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2024-09-11T01:45:12,282 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:42105)
2024-09-11T01:45:12,350 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:45:12,390 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (171.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (189.0 B) remote blocks
2024-09-11T01:45:12,396 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:43865 after 1 ms (0 ms spent in bootstraps)
2024-09-11T01:45:12,403 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 30 ms
2024-09-11T01:45:12,437 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 27.693589 ms
2024-09-11T01:45:12,515 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.269025 ms
2024-09-11T01:45:12,557 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.171441 ms
2024-09-11T01:45:12,576 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:45:12,577 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:45:12,634 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:45:12,648 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.0 in stage 4.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2024-09-11T01:45:12,685 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.0 in stage 4.0 (TID 4) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_4 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:45:12,694 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2024-09-11T01:45:12,694 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.1 in stage 4.0 (TID 5)
2024-09-11T01:45:12,702 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:45:12,706 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:45:12,716 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:32867 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:45:12,739 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.5 KiB, free 1663.1 MiB)
2024-09-11T01:45:12,744 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 37 ms
2024-09-11T01:45:12,746 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 232.2 KiB, free 1662.9 MiB)
2024-09-11T01:45:12,901 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2024-09-11T01:45:12,904 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:42105)
2024-09-11T01:45:12,925 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:45:12,960 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (171.0 B) remote blocks
2024-09-11T01:45:12,969 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 24 ms
2024-09-11T01:45:13,000 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 23.344802 ms
2024-09-11T01:45:13,084 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.240898 ms
2024-09-11T01:45:13,130 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 15.624842 ms
2024-09-11T01:45:13,149 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:45:13,149 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:45:13,213 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:45:13,231 [Executor task launch worker for task 0.1 in stage 4.0 (TID 5)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.1 in stage 4.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2024-09-11T01:45:13,255 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.1 in stage 4.0 (TID 5) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_5 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:45:13,259 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2024-09-11T01:45:13,260 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.2 in stage 4.0 (TID 6)
2024-09-11T01:45:13,310 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (171.0 B) remote blocks
2024-09-11T01:45:13,311 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 1 ms
2024-09-11T01:45:13,323 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:45:13,323 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:45:13,324 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:45:13,327 [Executor task launch worker for task 0.2 in stage 4.0 (TID 6)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.2 in stage 4.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2024-09-11T01:45:13,334 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.2 in stage 4.0 (TID 6) (172.19.0.5 executor 2): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_6 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/2)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:45:13,339 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 7
2024-09-11T01:45:13,340 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Running task 0.3 in stage 4.0 (TID 7)
2024-09-11T01:45:13,398 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (171.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (189.0 B) remote blocks
2024-09-11T01:45:13,399 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 2 ms
2024-09-11T01:45:13,414 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:45:13,415 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:45:13,416 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:45:13,418 [Executor task launch worker for task 0.3 in stage 4.0 (TID 7)] ERROR org.apache.spark.executor.Executor [] - Exception in task 0.3 in stage 4.0 (TID 7)
java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2024-09-11T01:45:13,426 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager [] - Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

2024-09-11T01:45:13,427 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager [] - Task 0 in stage 4.0 failed 4 times; aborting job
2024-09-11T01:45:13,435 [Thread-4] ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter [] - Aborting job 4e99b3a0-4d2e-499f-894c-8913e73741ff.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 1): java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461) ~[spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437) [spark-catalyst_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774) [spark-sql_2.12-3.5.2.jar:3.5.2]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:569) ~[?:?]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]
	at py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Mkdirs failed to create file:/opt/bitnami/spark/4961_stagemetrics/_temporary/0/_temporary/attempt_202409110145116145491329315028795_0004_m_000000_7 (exists=false, cwd=file:/opt/bitnami/spark/work/app-20240911014456-0003/1)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) ~[hadoop-client-api-3.3.4.jar:?]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.2.jar:3.5.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	... 1 more
2024-09-11T01:45:13,963 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:45:13,963 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:45:13,965 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:45:13,982 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:45:13,995 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:45:13,998 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:45:14,002 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:45:14,015 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:45:14,017 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:45:14,018 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:45:14,025 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:45:14,025 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:45:14,026 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:45:14,047 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:48:31,861 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:48:31,867 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:48:31,868 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:48:31,875 [Thread-4] WARN  org.apache.spark.SparkConf [] - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2024-09-11T01:48:31,913 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:31,914 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:48:31,915 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:31,915 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:48:31,949 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:48:31,961 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:48:31,962 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:48:32,036 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:48:32,037 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:48:32,038 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:32,039 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:32,039 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:48:32,128 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:48:32,441 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 39979.
2024-09-11T01:48:32,493 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:48:32,546 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:48:32,591 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:48:32,592 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:48:32,596 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:48:32,625 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-temp/blockmgr-2ea77e7f-a543-43d1-8b00-df9797e0c008
2024-09-11T01:48:32,653 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:48:32,674 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:48:32,748 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @4179ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:48:32,873 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:48:32,888 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:48:32,922 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4354ms
2024-09-11T01:48:32,975 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@1aaacdc3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:48:32,975 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:48:33,011 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@17138804{/,null,AVAILABLE,@Spark}
2024-09-11T01:48:33,157 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:48:33,301 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 66 ms (0 ms spent in bootstraps)
2024-09-11T01:48:33,565 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911014833-0004
2024-09-11T01:48:33,568 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014833-0004/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:48:33,574 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014833-0004/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:48:33,575 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014833-0004/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:48:33,576 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014833-0004/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:48:33,577 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911014833-0004/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:48:33,578 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911014833-0004/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:48:33,585 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42655.
2024-09-11T01:48:33,586 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:42655
2024-09-11T01:48:33,590 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:48:33,606 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 42655, None)
2024-09-11T01:48:33,614 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:42655 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 42655, None)
2024-09-11T01:48:33,619 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 42655, None)
2024-09-11T01:48:33,622 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 42655, None)
2024-09-11T01:48:33,694 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014833-0004/2 is now RUNNING
2024-09-11T01:48:33,699 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014833-0004/0 is now RUNNING
2024-09-11T01:48:33,702 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911014833-0004/1 is now RUNNING
2024-09-11T01:48:34,398 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911014833-0004.inprogress
2024-09-11T01:48:34,941 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@17138804{/,null,STOPPED,@Spark}
2024-09-11T01:48:34,957 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d91a830{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:48:34,962 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f573e6b{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:34,969 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1436a0c8{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:48:34,991 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2337a8f3{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,001 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73c76678{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,016 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2cdd02e0{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,026 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@45cd70fb{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,036 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@771d4abb{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,041 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@44575784{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,045 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@d89947d{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,052 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41c51df{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,057 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@373b93ec{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,063 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65e11e58{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,069 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ed72d9c{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,076 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@39bc0fd1{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,081 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28cfc219{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,087 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3312bb68{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,094 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1717c7cd{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,102 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@144f689c{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,109 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@cb7434e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,115 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5321809b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,122 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e9a2719{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,168 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@359101ef{/static,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,171 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1db593d4{/,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,186 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2bf5b155{/api,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,190 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4cd1328e{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,193 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6e035a86{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,234 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b99de6f{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:48:35,239 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:48:36,593 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 473@0c0fd82825b2
2024-09-11T01:48:36,608 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 496@d276954469b5
2024-09-11T01:48:36,613 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:48:36,616 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:48:36,616 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:48:36,627 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:48:36,629 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:48:36,630 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:48:36,833 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 497@e9a308ff8d57
2024-09-11T01:48:36,854 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:48:36,856 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:48:36,857 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:48:37,388 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:48:37,401 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:48:37,584 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:37,585 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:37,587 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:37,589 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:37,590 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:37,605 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:37,607 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:37,609 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:37,610 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:37,612 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:37,685 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:48:37,949 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:37,950 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:37,952 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:37,954 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:37,955 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:38,365 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 171 ms (0 ms spent in bootstraps)
2024-09-11T01:48:38,425 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 161 ms (0 ms spent in bootstraps)
2024-09-11T01:48:38,654 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:38,655 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:38,656 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:38,656 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:38,656 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:38,671 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:38,672 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:38,673 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:38,674 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:38,674 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:38,808 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 6 ms (0 ms spent in bootstraps)
2024-09-11T01:48:38,819 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 7 ms (0 ms spent in bootstraps)
2024-09-11T01:48:38,880 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 181 ms (0 ms spent in bootstraps)
2024-09-11T01:48:39,012 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5c92221-ba46-4e0a-bfb4-0aaf13cc2f0a/executor-b554327d-83db-4cc6-85ae-639008a92d4d/blockmgr-0b20810c-0fa1-48ea-be50-88965ef6ef2a
2024-09-11T01:48:39,012 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-0aee848a-ff36-45a7-94c2-67c781486a40/executor-f04e34bb-0512-4144-98e9-59db5750fd2f/blockmgr-6122a14d-31dc-40d1-a9ca-8f4955035e70
2024-09-11T01:48:39,088 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:48:39,094 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:48:39,209 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:48:39,220 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:48:39,221 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:48:39,222 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:48:39,222 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:48:39,361 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39979 after 7 ms (0 ms spent in bootstraps)
2024-09-11T01:48:39,575 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c0da433b-5299-4d08-a536-be15ca700c08/executor-c618c55c-b4d5-43a7-a73d-f1f940de6ada/blockmgr-55189b99-53a7-436f-a270-659bf59682cd
2024-09-11T01:48:39,578 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:39979
2024-09-11T01:48:39,580 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.3:43735
2024-09-11T01:48:39,602 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:39979
2024-09-11T01:48:39,604 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.5:45933
2024-09-11T01:48:39,610 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:43735 after 20 ms (0 ms spent in bootstraps)
2024-09-11T01:48:39,616 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.3:43735
2024-09-11T01:48:39,616 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:45933 after 7 ms (0 ms spent in bootstraps)
2024-09-11T01:48:39,618 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:39,625 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:48:39,627 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:39,635 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.5:45933
2024-09-11T01:48:39,646 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:39,647 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:48:39,648 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:39,688 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:48:39,734 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:48:39,736 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:48:39,742 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.19.0.5
2024-09-11T01:48:39,745 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:48:39,746 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:48:39,746 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.19.0.3
2024-09-11T01:48:39,749 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:48:39,750 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:48:39,846 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42371.
2024-09-11T01:48:39,846 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.5:42371
2024-09-11T01:48:39,852 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:48:39,857 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41059.
2024-09-11T01:48:39,858 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.3:41059
2024-09-11T01:48:39,865 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:48:39,872 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.19.0.5, 42371, None)
2024-09-11T01:48:39,881 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.19.0.3, 41059, None)
2024-09-11T01:48:39,897 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.19.0.5, 42371, None)
2024-09-11T01:48:39,900 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.19.0.3, 41059, None)
2024-09-11T01:48:39,903 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.19.0.3, 41059, None)
2024-09-11T01:48:39,900 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.19.0.5, 42371, None)
2024-09-11T01:48:39,918 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:48:39,918 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:48:39,920 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@26184d41 for default.
2024-09-11T01:48:39,920 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@233aa7ab for default.
2024-09-11T01:48:40,157 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:39979
2024-09-11T01:48:40,159 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.4:36373
2024-09-11T01:48:40,165 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:36373 after 4 ms (0 ms spent in bootstraps)
2024-09-11T01:48:40,169 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.4:36373
2024-09-11T01:48:40,177 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:40,178 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:48:40,179 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:48:40,214 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:48:40,219 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.19.0.4
2024-09-11T01:48:40,220 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:48:40,221 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:48:40,294 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37229.
2024-09-11T01:48:40,294 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.4:37229
2024-09-11T01:48:40,298 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:48:40,313 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.19.0.4, 37229, None)
2024-09-11T01:48:40,326 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.19.0.4, 37229, None)
2024-09-11T01:48:40,329 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.19.0.4, 37229, None)
2024-09-11T01:48:40,338 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:48:40,339 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@26184d41 for default.
2024-09-11T01:48:42,612 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2024-09-11T01:48:42,621 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2024-09-11T01:48:42,727 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:42,793 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42655 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:48:42,858 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:48:42,871 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 144 ms
2024-09-11T01:48:42,942 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:48:44,668 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 321.105693 ms
2024-09-11T01:48:44,679 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1177, boot = 929, init = 248, finish = 0
2024-09-11T01:48:44,704 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2595 bytes result sent to driver
2024-09-11T01:48:44,783 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2024-09-11T01:48:44,784 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2024-09-11T01:48:44,801 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:44,804 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2024-09-11T01:48:44,826 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2024-09-11T01:48:44,847 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 1.0 (TID 2)
2024-09-11T01:48:44,866 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 2.0 in stage 1.0 (TID 3)
2024-09-11T01:48:44,867 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:48:44,876 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 74 ms
2024-09-11T01:48:44,881 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:48:45,086 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:45,101 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:45,116 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 222, boot = -469, init = 690, finish = 1
2024-09-11T01:48:45,127 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
2024-09-11T01:48:45,218 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:41059 after 4 ms (0 ms spent in bootstraps)
2024-09-11T01:48:45,234 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42655 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:48:45,297 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:48:45,315 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 228 ms
2024-09-11T01:48:45,346 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1663.2 MiB)
2024-09-11T01:48:45,387 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 285 ms
2024-09-11T01:48:45,482 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:48:45,550 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.3 KiB, free 1663.2 MiB)
2024-09-11T01:48:47,901 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 444.028801 ms
2024-09-11T01:48:47,916 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1388, boot = 1091, init = 296, finish = 1
2024-09-11T01:48:47,919 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 371.283134 ms
2024-09-11T01:48:47,933 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1370, boot = 1122, init = 248, finish = 0
2024-09-11T01:48:47,956 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 1.0 (TID 2). 2581 bytes result sent to driver
2024-09-11T01:48:47,971 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 2.0 in stage 1.0 (TID 3). 2714 bytes result sent to driver
2024-09-11T01:48:49,612 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:48:49,623 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:48:50,000 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2024-09-11T01:48:50,001 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2024-09-11T01:48:50,001 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 2.0 (TID 4)
2024-09-11T01:48:50,001 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 2.0 (TID 5)
2024-09-11T01:48:50,012 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:50,019 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:50,032 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:48:50,032 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:42655 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:48:50,037 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 24 ms
2024-09-11T01:48:50,040 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:48:50,052 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:48:50,060 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 40 ms
2024-09-11T01:48:50,064 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:48:50,246 [Executor task launch worker for task 1.0 in stage 2.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 2.0 (TID 5). 1708 bytes result sent to driver
2024-09-11T01:48:50,272 [Executor task launch worker for task 0.0 in stage 2.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 2.0 (TID 4). 1708 bytes result sent to driver
2024-09-11T01:48:50,623 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2024-09-11T01:48:50,624 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 6)
2024-09-11T01:48:50,632 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:48:50,635 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:50,647 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.5 KiB, free 1663.1 MiB)
2024-09-11T01:48:50,651 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 15 ms
2024-09-11T01:48:50,654 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 232.2 KiB, free 1662.9 MiB)
2024-09-11T01:48:50,831 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2024-09-11T01:48:50,833 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:39979)
2024-09-11T01:48:50,901 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:48:50,938 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (171.0 B) remote blocks
2024-09-11T01:48:50,946 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:42371 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:48:50,954 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 31 ms
2024-09-11T01:48:50,994 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 30.820785 ms
2024-09-11T01:48:51,092 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 19.929891 ms
2024-09-11T01:48:51,136 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 10.36694 ms
2024-09-11T01:48:51,160 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:48:51,160 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:48:51,223 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:48:51,389 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110148505750989083851088856_0004_m_000000_6' to file:/opt/bitnami/spark/5583_stagemetrics/_temporary/0/task_202409110148505750989083851088856_0004_m_000000
2024-09-11T01:48:51,390 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110148505750989083851088856_0004_m_000000_6: Committed. Elapsed time: 1 ms.
2024-09-11T01:48:51,412 [Executor task launch worker for task 0.0 in stage 4.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 6). 5308 bytes result sent to driver
2024-09-11T01:48:52,025 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 7
2024-09-11T01:48:52,026 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 5.0 (TID 7)
2024-09-11T01:48:52,027 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 8
2024-09-11T01:48:52,028 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 5.0 (TID 8)
2024-09-11T01:48:52,031 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:48:52,037 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:52,038 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:48:52,043 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:52,051 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:48:52,057 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:48:52,060 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 22 ms
2024-09-11T01:48:52,062 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 18 ms
2024-09-11T01:48:52,064 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:48:52,066 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:48:52,278 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 200.4825 ms
2024-09-11T01:48:52,291 [Executor task launch worker for task 0.0 in stage 5.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 5.0 (TID 7). 1835 bytes result sent to driver
2024-09-11T01:48:52,344 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 151.758723 ms
2024-09-11T01:48:52,404 [Executor task launch worker for task 1.0 in stage 5.0 (TID 8)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 5.0 (TID 8). 1921 bytes result sent to driver
2024-09-11T01:48:52,705 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 9
2024-09-11T01:48:52,706 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 7.0 (TID 9)
2024-09-11T01:48:52,718 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 2 and clearing cache
2024-09-11T01:48:52,722 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:48:52,733 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 90.3 KiB, free 1663.1 MiB)
2024-09-11T01:48:52,738 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 16 ms
2024-09-11T01:48:52,742 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 269.2 KiB, free 1662.8 MiB)
2024-09-11T01:48:52,899 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 1, fetching them
2024-09-11T01:48:52,901 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:39979)
2024-09-11T01:48:52,936 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:48:52,984 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (246.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (117.0 B) remote blocks
2024-09-11T01:48:52,991 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:42371 after 1 ms (0 ms spent in bootstraps)
2024-09-11T01:48:52,999 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 36 ms
2024-09-11T01:48:53,105 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 98.757289 ms
2024-09-11T01:48:53,115 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:48:53,115 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:48:53,187 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:48:53,315 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110148525778706337617601553_0007_m_000000_9' to file:/opt/bitnami/spark/5583_stagemetrics_summary/_temporary/0/task_202409110148525778706337617601553_0007_m_000000
2024-09-11T01:48:53,317 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110148525778706337617601553_0007_m_000000_9: Committed. Elapsed time: 1 ms.
2024-09-11T01:48:53,326 [Executor task launch worker for task 0.0 in stage 7.0 (TID 9)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 7.0 (TID 9). 5017 bytes result sent to driver
2024-09-11T01:48:53,507 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:48:53,508 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:48:53,509 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:48:53,541 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:48:53,545 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:48:53,547 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:48:53,552 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:48:53,556 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:48:53,562 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:48:53,566 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:48:53,563 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:48:53,567 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:48:53,569 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:48:53,587 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:48:53,594 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:50:11,051 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:50:11,056 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:50:11,057 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:50:11,062 [Thread-4] WARN  org.apache.spark.SparkConf [] - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2024-09-11T01:50:11,098 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:11,099 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:50:11,099 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:11,100 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:50:11,132 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:50:11,142 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:50:11,144 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:50:11,227 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:50:11,228 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:50:11,229 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:11,229 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:11,230 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:50:11,305 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:50:11,648 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 38721.
2024-09-11T01:50:11,685 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:50:11,735 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:50:11,774 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:50:11,776 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:50:11,782 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:50:11,819 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-temp/blockmgr-7b36ef25-eb75-408b-8da1-7047790a9a88
2024-09-11T01:50:11,843 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:50:11,862 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:50:11,918 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3837ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:50:12,020 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:50:12,034 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:50:12,057 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @3977ms
2024-09-11T01:50:12,105 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@4136ccc3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:50:12,106 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:50:12,139 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5c8a29a5{/,null,AVAILABLE,@Spark}
2024-09-11T01:50:12,277 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:50:12,339 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2024-09-11T01:50:12,465 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911015012-0005
2024-09-11T01:50:12,474 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015012-0005/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:50:12,483 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015012-0005/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:50:12,484 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015012-0005/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:50:12,485 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015012-0005/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:50:12,487 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015012-0005/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:50:12,488 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015012-0005/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:50:12,493 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34423.
2024-09-11T01:50:12,494 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:34423
2024-09-11T01:50:12,496 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:50:12,508 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 34423, None)
2024-09-11T01:50:12,514 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:34423 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 34423, None)
2024-09-11T01:50:12,520 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 34423, None)
2024-09-11T01:50:12,523 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 34423, None)
2024-09-11T01:50:12,589 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015012-0005/0 is now RUNNING
2024-09-11T01:50:12,590 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015012-0005/2 is now RUNNING
2024-09-11T01:50:12,596 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015012-0005/1 is now RUNNING
2024-09-11T01:50:13,002 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911015012-0005.inprogress
2024-09-11T01:50:13,296 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@5c8a29a5{/,null,STOPPED,@Spark}
2024-09-11T01:50:13,299 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@452430a9{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,305 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4a4e32d3{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,310 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8425768{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@42fbf39a{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,323 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6298abdb{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,340 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65c2f626{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,345 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e65803{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,351 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a6a584e{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,353 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@18809651{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,356 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22fa189{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,364 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3e9f5240{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,373 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@129a6c85{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,375 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7f1977d5{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,382 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@66629902{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,384 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@68661c20{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,386 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5414a5a1{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,389 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7b34e799{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,391 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4865baaf{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,396 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c648763{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,404 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59000787{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,410 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@24a4e920{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,415 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a90cc70{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,453 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c9ab33b{/static,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,456 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4f2c078d{/,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,468 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15d9a953{/api,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,473 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@62547423{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,486 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1463ce8b{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,504 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c6d9f8{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:50:13,506 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:50:16,287 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 593@0c0fd82825b2
2024-09-11T01:50:16,323 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:50:16,325 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:50:16,326 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:50:16,667 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 595@d276954469b5
2024-09-11T01:50:16,688 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:50:16,690 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:50:16,691 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:50:16,710 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 611@e9a308ff8d57
2024-09-11T01:50:16,736 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:50:16,738 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:50:16,738 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:50:17,428 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:50:17,835 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:17,838 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:17,841 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:17,843 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:17,846 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:18,017 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:50:18,068 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:50:18,341 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:18,342 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:18,344 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:18,345 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:18,347 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:18,445 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:18,447 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:18,450 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:18,452 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:18,454 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:18,848 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 161 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,088 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:19,089 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:19,090 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:19,090 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:19,091 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:19,241 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 180 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,249 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 8 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,254 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 204 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,480 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-0aee848a-ff36-45a7-94c2-67c781486a40/executor-fea3d596-3700-441d-b876-e001b5088976/blockmgr-90cdb3b8-9516-4367-b344-7e59a823d6a5
2024-09-11T01:50:19,531 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:19,532 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:19,534 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:19,534 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:19,536 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:19,572 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:50:19,594 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:50:19,594 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:50:19,598 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:50:19,599 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:50:19,600 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:50:19,758 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 9 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,801 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:38721 after 9 ms (0 ms spent in bootstraps)
2024-09-11T01:50:19,967 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c0da433b-5299-4d08-a536-be15ca700c08/executor-3af1a25d-614e-47fc-aef2-028b221cdf37/blockmgr-b07ae2c6-7ad1-4760-9180-abe3f0bb4dcf
2024-09-11T01:50:20,014 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5c92221-ba46-4e0a-bfb4-0aaf13cc2f0a/executor-3cbe3733-f194-4bf1-99e7-ceed0dc41a6c/blockmgr-7aeca1a5-8a0c-4152-90f8-b52750f4e8c8
2024-09-11T01:50:20,047 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:50:20,112 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:50:20,200 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:38721
2024-09-11T01:50:20,205 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.3:43735
2024-09-11T01:50:20,221 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:43735 after 11 ms (0 ms spent in bootstraps)
2024-09-11T01:50:20,224 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.3:43735
2024-09-11T01:50:20,226 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,227 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:50:20,228 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,298 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:50:20,307 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.19.0.3
2024-09-11T01:50:20,309 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:50:20,315 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:50:20,401 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32935.
2024-09-11T01:50:20,401 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.3:32935
2024-09-11T01:50:20,409 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:50:20,426 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.19.0.3, 32935, None)
2024-09-11T01:50:20,489 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.19.0.3, 32935, None)
2024-09-11T01:50:20,492 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.19.0.3, 32935, None)
2024-09-11T01:50:20,531 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:50:20,535 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@53bb8296 for default.
2024-09-11T01:50:20,675 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:38721
2024-09-11T01:50:20,677 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.4:36373
2024-09-11T01:50:20,691 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:38721
2024-09-11T01:50:20,695 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.5:45933
2024-09-11T01:50:20,706 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:45933 after 5 ms (0 ms spent in bootstraps)
2024-09-11T01:50:20,706 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:36373 after 23 ms (0 ms spent in bootstraps)
2024-09-11T01:50:20,713 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,713 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.5:45933
2024-09-11T01:50:20,714 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:50:20,716 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,712 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.4:36373
2024-09-11T01:50:20,740 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,741 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:50:20,742 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:50:20,772 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:50:20,779 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.19.0.4
2024-09-11T01:50:20,781 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:50:20,783 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:50:20,798 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:50:20,805 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.19.0.5
2024-09-11T01:50:20,807 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:50:20,808 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:50:20,873 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35617.
2024-09-11T01:50:20,874 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.4:35617
2024-09-11T01:50:20,878 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:50:20,891 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.19.0.4, 35617, None)
2024-09-11T01:50:20,893 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36421.
2024-09-11T01:50:20,894 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.5:36421
2024-09-11T01:50:20,901 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:50:20,916 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.19.0.4, 35617, None)
2024-09-11T01:50:20,918 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.19.0.5, 36421, None)
2024-09-11T01:50:20,919 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.19.0.4, 35617, None)
2024-09-11T01:50:20,935 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:50:20,937 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@355f4eaf for default.
2024-09-11T01:50:20,939 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.19.0.5, 36421, None)
2024-09-11T01:50:20,942 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.19.0.5, 36421, None)
2024-09-11T01:50:20,958 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:50:20,961 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@53bb8296 for default.
2024-09-11T01:50:21,700 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2024-09-11T01:50:21,710 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2024-09-11T01:50:21,815 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:21,864 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:34423 after 1 ms (0 ms spent in bootstraps)
2024-09-11T01:50:21,916 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 1663.2 MiB)
2024-09-11T01:50:21,926 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 110 ms
2024-09-11T01:50:21,986 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 13.8 KiB, free 1663.2 MiB)
2024-09-11T01:50:24,133 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 440.822299 ms
2024-09-11T01:50:24,149 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1490, boot = 1191, init = 299, finish = 0
2024-09-11T01:50:24,187 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2549 bytes result sent to driver
2024-09-11T01:50:24,284 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2024-09-11T01:50:24,303 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2024-09-11T01:50:24,458 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:24,541 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:34423 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:50:24,577 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 1663.2 MiB)
2024-09-11T01:50:24,587 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 129 ms
2024-09-11T01:50:24,663 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 13.8 KiB, free 1663.2 MiB)
2024-09-11T01:50:26,695 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 277.658168 ms
2024-09-11T01:50:26,709 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1411, boot = 1226, init = 185, finish = 0
2024-09-11T01:50:26,742 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2549 bytes result sent to driver
2024-09-11T01:50:28,183 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:50:28,194 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:50:28,687 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2024-09-11T01:50:28,688 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 2.0 (TID 2)
2024-09-11T01:50:28,689 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2024-09-11T01:50:28,690 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 2.0 (TID 3)
2024-09-11T01:50:28,705 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:28,708 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:28,722 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:50:28,734 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 28 ms
2024-09-11T01:50:28,740 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:50:28,741 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:50:28,753 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 44 ms
2024-09-11T01:50:28,761 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:50:28,944 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 2.0 (TID 2). 1708 bytes result sent to driver
2024-09-11T01:50:28,948 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 2.0 (TID 3). 1665 bytes result sent to driver
2024-09-11T01:50:29,265 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2024-09-11T01:50:29,266 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 4)
2024-09-11T01:50:29,276 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:50:29,280 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:29,290 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.5 KiB, free 1663.1 MiB)
2024-09-11T01:50:29,294 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 13 ms
2024-09-11T01:50:29,298 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 232.2 KiB, free 1662.9 MiB)
2024-09-11T01:50:29,534 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2024-09-11T01:50:29,538 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:38721)
2024-09-11T01:50:29,683 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:50:29,788 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (171.0 B) remote blocks
2024-09-11T01:50:29,798 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:32935 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:50:29,811 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 58 ms
2024-09-11T01:50:29,864 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 42.673936 ms
2024-09-11T01:50:30,011 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 25.238544 ms
2024-09-11T01:50:30,091 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 26.217992 ms
2024-09-11T01:50:30,136 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:50:30,137 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:50:30,270 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:50:30,467 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110150297116534978943224287_0004_m_000000_4' to file:/opt/bitnami/spark/5253_stagemetrics/_temporary/0/task_202409110150297116534978943224287_0004_m_000000
2024-09-11T01:50:30,469 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110150297116534978943224287_0004_m_000000_4: Committed. Elapsed time: 2 ms.
2024-09-11T01:50:30,501 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 4). 5308 bytes result sent to driver
2024-09-11T01:50:31,590 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2024-09-11T01:50:31,594 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 5.0 (TID 6)
2024-09-11T01:50:31,609 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:50:31,612 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2024-09-11T01:50:31,625 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:31,635 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 5.0 (TID 5)
2024-09-11T01:50:31,644 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:50:31,655 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 30 ms
2024-09-11T01:50:31,659 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:50:31,726 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:50:31,833 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:31,921 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 240.432447 ms
2024-09-11T01:50:31,937 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 5.0 (TID 6). 1835 bytes result sent to driver
2024-09-11T01:50:31,967 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:34423 after 3 ms (0 ms spent in bootstraps)
2024-09-11T01:50:32,005 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:50:32,017 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 183 ms
2024-09-11T01:50:32,094 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:50:33,027 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 550.999132 ms
2024-09-11T01:50:33,093 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 5.0 (TID 5). 1921 bytes result sent to driver
2024-09-11T01:50:33,324 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 7
2024-09-11T01:50:33,325 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 7.0 (TID 7)
2024-09-11T01:50:33,334 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 2 and clearing cache
2024-09-11T01:50:33,339 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:50:33,350 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 90.3 KiB, free 1663.1 MiB)
2024-09-11T01:50:33,354 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 14 ms
2024-09-11T01:50:33,358 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 269.2 KiB, free 1662.8 MiB)
2024-09-11T01:50:33,511 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 1, fetching them
2024-09-11T01:50:33,515 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:38721)
2024-09-11T01:50:33,550 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:50:33,597 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (234.0 B) non-empty blocks including 1 (117.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (117.0 B) remote blocks
2024-09-11T01:50:33,606 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:32935 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:50:33,613 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 35 ms
2024-09-11T01:50:33,725 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 104.772897 ms
2024-09-11T01:50:33,734 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:50:33,735 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:50:33,802 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:50:33,941 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110150334779216300019930525_0007_m_000000_7' to file:/opt/bitnami/spark/5253_stagemetrics_summary/_temporary/0/task_202409110150334779216300019930525_0007_m_000000
2024-09-11T01:50:33,943 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110150334779216300019930525_0007_m_000000_7: Committed. Elapsed time: 2 ms.
2024-09-11T01:50:33,955 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 7.0 (TID 7). 5060 bytes result sent to driver
2024-09-11T01:50:34,153 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:50:34,153 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:50:34,154 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:50:34,192 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:50:34,192 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:50:34,195 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:50:34,197 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:50:34,193 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:50:34,208 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:50:34,210 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:50:34,208 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:50:34,213 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:50:34,216 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:50:34,215 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:50:34,218 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:51:58,317 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.2
2024-09-11T01:51:58,321 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:51:58,322 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.12
2024-09-11T01:51:58,330 [Thread-4] WARN  org.apache.spark.SparkConf [] - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2024-09-11T01:51:58,363 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:51:58,364 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2024-09-11T01:51:58,365 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:51:58,366 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: elt-app
2024-09-11T01:51:58,401 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-11T01:51:58,420 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2024-09-11T01:51:58,421 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2024-09-11T01:51:58,483 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2024-09-11T01:51:58,484 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2024-09-11T01:51:58,485 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:51:58,486 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:51:58,487 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2024-09-11T01:51:58,575 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:51:58,881 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 40175.
2024-09-11T01:51:58,920 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2024-09-11T01:51:58,972 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2024-09-11T01:51:59,016 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-11T01:51:59,018 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2024-09-11T01:51:59,025 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2024-09-11T01:51:59,067 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-temp/blockmgr-c87cc6c3-0aa4-4107-ac87-14f680a63f13
2024-09-11T01:51:59,089 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2024-09-11T01:51:59,112 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2024-09-11T01:51:59,170 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @3867ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-11T01:51:59,294 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-11T01:51:59,306 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.12+10-LTS
2024-09-11T01:51:59,329 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @4027ms
2024-09-11T01:51:59,369 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@7315df3e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-11T01:51:59,370 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2024-09-11T01:51:59,403 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d62b92d{/,null,AVAILABLE,@Spark}
2024-09-11T01:51:59,538 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2024-09-11T01:51:59,597 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.19.0.2:7077 after 37 ms (0 ms spent in bootstraps)
2024-09-11T01:51:59,699 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20240911015159-0006
2024-09-11T01:51:59,711 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015159-0006/0 on worker-20240911013318-172.19.0.3-43735 (172.19.0.3:43735) with 2 core(s)
2024-09-11T01:51:59,716 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015159-0006/0 on hostPort 172.19.0.3:43735 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:51:59,720 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015159-0006/1 on worker-20240911013321-172.19.0.4-36373 (172.19.0.4:36373) with 2 core(s)
2024-09-11T01:51:59,721 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015159-0006/1 on hostPort 172.19.0.4:36373 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:51:59,724 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39549.
2024-09-11T01:51:59,725 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20240911015159-0006/2 on worker-20240911013323-172.19.0.5-45933 (172.19.0.5:45933) with 2 core(s)
2024-09-11T01:51:59,726 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 804d151ea7c0:39549
2024-09-11T01:51:59,726 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20240911015159-0006/2 on hostPort 172.19.0.5:45933 with 2 core(s), 3.0 GiB RAM
2024-09-11T01:51:59,730 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:51:59,746 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, 804d151ea7c0, 39549, None)
2024-09-11T01:51:59,753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 804d151ea7c0:39549 with 434.4 MiB RAM, BlockManagerId(driver, 804d151ea7c0, 39549, None)
2024-09-11T01:51:59,759 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, 804d151ea7c0, 39549, None)
2024-09-11T01:51:59,762 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, 804d151ea7c0, 39549, None)
2024-09-11T01:51:59,855 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015159-0006/1 is now RUNNING
2024-09-11T01:51:59,858 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015159-0006/0 is now RUNNING
2024-09-11T01:51:59,863 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20240911015159-0006/2 is now RUNNING
2024-09-11T01:52:00,395 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20240911015159-0006.inprogress
2024-09-11T01:52:00,741 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@6d62b92d{/,null,STOPPED,@Spark}
2024-09-11T01:52:00,756 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@a2f07b1{/jobs,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,761 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5bbb961b{/jobs/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,777 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5add680e{/jobs/job,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,784 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2cc9e290{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,794 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49d7b5cf{/stages,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,810 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6f0cbe66{/stages/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,824 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@350433d9{/stages/stage,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,842 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@14f95d22{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,845 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@380b75eb{/stages/pool,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,858 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5b4d9b4b{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,873 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3c01441c{/storage,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,912 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@108dc213{/storage/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,942 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4f046b6c{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,961 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a1279c7{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,978 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@705bfcc8{/environment,null,AVAILABLE,@Spark}
2024-09-11T01:52:00,981 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1066cfc1{/environment/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,015 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@661c4f81{/executors,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,033 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@237981a4{/executors/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,039 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c1676b0{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,052 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4c051d40{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,068 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@69622a21{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,070 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58c3c183{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,136 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@156ea221{/static,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,140 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@134c4590{/,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,157 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@70635a09{/api,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,163 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d10fdc1{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,171 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@75196555{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,185 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cd6ddeb{/metrics/json,null,AVAILABLE,@Spark}
2024-09-11T01:52:01,187 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2024-09-11T01:52:04,503 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 720@e9a308ff8d57
2024-09-11T01:52:04,530 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:52:04,532 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:52:04,532 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:52:04,553 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 714@d276954469b5
2024-09-11T01:52:04,563 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 698@0c0fd82825b2
2024-09-11T01:52:04,570 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:52:04,572 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:52:04,572 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:52:04,582 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2024-09-11T01:52:04,584 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2024-09-11T01:52:04,584 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2024-09-11T01:52:06,500 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:52:06,556 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:52:06,575 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-11T01:52:07,040 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:07,043 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:07,046 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:07,047 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:07,051 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:07,210 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:07,213 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:07,219 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:07,220 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:07,221 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:07,218 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:07,228 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:07,230 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:07,237 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:07,242 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:08,140 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 211 ms (0 ms spent in bootstraps)
2024-09-11T01:52:08,214 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 169 ms (0 ms spent in bootstraps)
2024-09-11T01:52:08,384 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 232 ms (0 ms spent in bootstraps)
2024-09-11T01:52:08,479 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:08,480 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:08,482 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:08,482 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:08,484 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:08,562 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:08,563 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:08,564 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:08,565 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:08,565 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:08,682 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 4 ms (0 ms spent in bootstraps)
2024-09-11T01:52:08,782 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2024-09-11T01:52:08,783 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2024-09-11T01:52:08,787 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2024-09-11T01:52:08,788 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2024-09-11T01:52:08,788 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2024-09-11T01:52:08,812 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 9 ms (0 ms spent in bootstraps)
2024-09-11T01:52:08,905 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-0aee848a-ff36-45a7-94c2-67c781486a40/executor-f05a3142-7ab7-4805-a06e-90197261655d/blockmgr-a32568bb-b2e8-4dc2-8410-93ec007ba27e
2024-09-11T01:52:08,997 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:40175 after 16 ms (0 ms spent in bootstraps)
2024-09-11T01:52:09,001 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:52:09,073 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c0da433b-5299-4d08-a536-be15ca700c08/executor-8fc85286-96d2-491e-b657-58ce2b8214cd/blockmgr-330694fb-a207-4c74-b009-3c2a5e7730e7
2024-09-11T01:52:09,157 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:52:09,229 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-c5c92221-ba46-4e0a-bfb4-0aaf13cc2f0a/executor-a30c1aa9-b11c-4481-ba46-7d2afb17e933/blockmgr-e446d20a-8ba1-420e-a8a6-2fcc78dbde50
2024-09-11T01:52:09,352 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 1663.2 MiB
2024-09-11T01:52:09,709 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:40175
2024-09-11T01:52:09,716 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.3:43735
2024-09-11T01:52:09,748 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.4:36373
2024-09-11T01:52:09,749 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:40175
2024-09-11T01:52:09,756 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:09,759 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:52:09,757 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.3:43735 after 27 ms (0 ms spent in bootstraps)
2024-09-11T01:52:09,761 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:09,781 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.3:43735
2024-09-11T01:52:09,815 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.4:36373
2024-09-11T01:52:09,812 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:36373 after 57 ms (0 ms spent in bootstraps)
2024-09-11T01:52:09,818 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:09,819 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:52:09,823 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:09,885 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:52:09,889 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:52:09,895 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.19.0.3
2024-09-11T01:52:09,896 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.19.0.4
2024-09-11T01:52:09,896 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:52:09,897 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:52:09,898 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:52:09,898 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:52:09,993 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37909.
2024-09-11T01:52:09,994 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.3:37909
2024-09-11T01:52:09,998 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:52:10,006 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39557.
2024-09-11T01:52:10,007 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.4:39557
2024-09-11T01:52:10,018 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.19.0.3, 37909, None)
2024-09-11T01:52:10,016 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:52:10,053 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@804d151ea7c0:40175
2024-09-11T01:52:10,059 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.19.0.4, 39557, None)
2024-09-11T01:52:10,057 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.19.0.5:45933
2024-09-11T01:52:10,063 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.19.0.3, 37909, None)
2024-09-11T01:52:10,070 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.19.0.3, 37909, None)
2024-09-11T01:52:10,094 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:10,096 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2024-09-11T01:52:10,098 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2024-09-11T01:52:10,098 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.19.0.4, 39557, None)
2024-09-11T01:52:10,101 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:52:10,102 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.19.0.4, 39557, None)
2024-09-11T01:52:10,104 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4b71ed99 for default.
2024-09-11T01:52:10,121 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:52:10,124 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5e89724c for default.
2024-09-11T01:52:10,125 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.5:45933 after 47 ms (0 ms spent in bootstraps)
2024-09-11T01:52:10,137 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.19.0.5:45933
2024-09-11T01:52:10,197 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2024-09-11T01:52:10,204 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.19.0.5
2024-09-11T01:52:10,206 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.8.0-40-generic, amd64
2024-09-11T01:52:10,207 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.12
2024-09-11T01:52:10,267 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43761.
2024-09-11T01:52:10,267 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.19.0.5:43761
2024-09-11T01:52:10,270 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-11T01:52:10,282 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.19.0.5, 43761, None)
2024-09-11T01:52:10,297 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.19.0.5, 43761, None)
2024-09-11T01:52:10,300 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.19.0.5, 43761, None)
2024-09-11T01:52:10,312 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-11T01:52:10,314 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@53bb8296 for default.
2024-09-11T01:52:11,461 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2024-09-11T01:52:11,475 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2024-09-11T01:52:11,630 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:11,712 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39549 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:52:11,769 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 1663.2 MiB)
2024-09-11T01:52:11,780 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 149 ms
2024-09-11T01:52:11,847 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 13.8 KiB, free 1663.2 MiB)
2024-09-11T01:52:13,615 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 290.431326 ms
2024-09-11T01:52:13,626 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1246, boot = 983, init = 262, finish = 1
2024-09-11T01:52:13,651 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2542 bytes result sent to driver
2024-09-11T01:52:13,833 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2024-09-11T01:52:13,873 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2024-09-11T01:52:14,002 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:14,068 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39549 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:52:14,106 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 1663.2 MiB)
2024-09-11T01:52:14,116 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 114 ms
2024-09-11T01:52:14,202 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 13.8 KiB, free 1663.2 MiB)
2024-09-11T01:52:16,735 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 444.997069 ms
2024-09-11T01:52:16,747 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.api.python.PythonRunner [] - Times: total = 1626, boot = 1325, init = 301, finish = 0
2024-09-11T01:52:16,773 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2580 bytes result sent to driver
2024-09-11T01:52:18,692 [Thread-4] WARN  org.apache.spark.sql.catalyst.util.SparkStringUtils [] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2024-09-11T01:52:18,702 [Thread-4] WARN  ch.cern.sparkmeasure.StageMetrics [] - Stage metrics data refreshed into temp view PerfStageMetrics
2024-09-11T01:52:19,057 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2024-09-11T01:52:19,058 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 2.0 (TID 2)
2024-09-11T01:52:19,069 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2024-09-11T01:52:19,071 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:19,087 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:52:19,090 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 2.0 (TID 3)
2024-09-11T01:52:19,094 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 22 ms
2024-09-11T01:52:19,098 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:52:19,256 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:19,271 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 2.0 (TID 2). 1708 bytes result sent to driver
2024-09-11T01:52:19,346 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to 804d151ea7c0/172.19.0.2:39549 after 7 ms (0 ms spent in bootstraps)
2024-09-11T01:52:19,413 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 1663.2 MiB)
2024-09-11T01:52:19,433 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 176 ms
2024-09-11T01:52:19,504 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 10.4 KiB, free 1663.2 MiB)
2024-09-11T01:52:19,735 [Executor task launch worker for task 1.0 in stage 2.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 2.0 (TID 3). 1708 bytes result sent to driver
2024-09-11T01:52:20,005 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 4
2024-09-11T01:52:20,006 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 4)
2024-09-11T01:52:20,019 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:52:20,025 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:20,041 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.5 KiB, free 1663.1 MiB)
2024-09-11T01:52:20,048 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 22 ms
2024-09-11T01:52:20,056 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 232.2 KiB, free 1662.9 MiB)
2024-09-11T01:52:20,300 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2024-09-11T01:52:20,303 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:40175)
2024-09-11T01:52:20,363 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:52:20,409 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (360.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (171.0 B) remote blocks
2024-09-11T01:52:20,417 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.19.0.4:39557 after 2 ms (0 ms spent in bootstraps)
2024-09-11T01:52:20,426 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 39 ms
2024-09-11T01:52:21,018 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 358.164311 ms
2024-09-11T01:52:21,150 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 21.862146 ms
2024-09-11T01:52:21,243 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 17.820074 ms
2024-09-11T01:52:21,275 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:52:21,276 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:52:21,349 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:52:21,609 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110152196466549449906973251_0004_m_000000_4' to file:/opt/bitnami/spark/5599_stagemetrics/_temporary/0/task_202409110152196466549449906973251_0004_m_000000
2024-09-11T01:52:21,610 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110152196466549449906973251_0004_m_000000_4: Committed. Elapsed time: 2 ms.
2024-09-11T01:52:21,640 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 4). 5308 bytes result sent to driver
2024-09-11T01:52:22,348 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 6
2024-09-11T01:52:22,348 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 5
2024-09-11T01:52:22,349 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Running task 1.0 in stage 5.0 (TID 6)
2024-09-11T01:52:22,349 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 5.0 (TID 5)
2024-09-11T01:52:22,354 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2024-09-11T01:52:22,359 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:22,369 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:52:22,374 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 15 ms
2024-09-11T01:52:22,377 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:52:22,379 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:22,425 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 1663.2 MiB)
2024-09-11T01:52:22,432 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 52 ms
2024-09-11T01:52:22,436 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 44.3 KiB, free 1663.1 MiB)
2024-09-11T01:52:22,613 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 226.78695 ms
2024-09-11T01:52:22,639 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 5.0 (TID 5). 1835 bytes result sent to driver
2024-09-11T01:52:22,681 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 227.306394 ms
2024-09-11T01:52:22,692 [Executor task launch worker for task 1.0 in stage 5.0 (TID 6)] INFO  org.apache.spark.executor.Executor [] - Finished task 1.0 in stage 5.0 (TID 6). 1835 bytes result sent to driver
2024-09-11T01:52:23,061 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 7
2024-09-11T01:52:23,063 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 7.0 (TID 7)
2024-09-11T01:52:23,066 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 2 and clearing cache
2024-09-11T01:52:23,070 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2024-09-11T01:52:23,087 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 90.3 KiB, free 1663.1 MiB)
2024-09-11T01:52:23,093 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 23 ms
2024-09-11T01:52:23,097 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 269.2 KiB, free 1662.8 MiB)
2024-09-11T01:52:23,131 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 1, fetching them
2024-09-11T01:52:23,131 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@804d151ea7c0:40175)
2024-09-11T01:52:23,138 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2024-09-11T01:52:23,141 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 2 (246.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 1 (117.0 B) remote blocks
2024-09-11T01:52:23,142 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 1 remote fetches in 3 ms
2024-09-11T01:52:23,324 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 180.820927 ms
2024-09-11T01:52:23,330 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - File Output Committer Algorithm version is 1
2024-09-11T01:52:23,330 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-11T01:52:23,332 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol [] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-09-11T01:52:23,394 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter [] - Saved output of task 'attempt_202409110152225679886174643040094_0007_m_000000_7' to file:/opt/bitnami/spark/5599_stagemetrics_summary/_temporary/0/task_202409110152225679886174643040094_0007_m_000000
2024-09-11T01:52:23,395 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil [] - attempt_202409110152225679886174643040094_0007_m_000000_7: Committed. Elapsed time: 3 ms.
2024-09-11T01:52:23,403 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 7.0 (TID 7). 5017 bytes result sent to driver
2024-09-11T01:52:23,646 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:52:23,646 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:52:23,647 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2024-09-11T01:52:23,690 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:52:23,698 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:52:23,695 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:52:23,702 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:52:23,708 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2024-09-11T01:52:23,707 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:52:23,708 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:52:23,712 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2024-09-11T01:52:23,739 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:52:23,742 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2024-09-11T01:52:23,743 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2024-09-11T01:52:23,747 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
